{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# HSE Summer School NLP&DA\n",
    "\n",
    "![](https://www.hse.ru/data/2014/06/25/1309038576/logo_hse_cmyk_e.jpg)\n",
    "\n",
    "Учебный проект по теме \"Тональность отношений субъектов (именованных сущностей)\", предполагающий разработку участниками школы под руководством тьюторов программных средств, решающих задачу определения мнения сторон о различных событиях, освещаемых в новостях"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужен алгоритм, который по выделенным сущностям может найти пары этих сущностей в тексте и вырезать соответствующие куски между ними + несколько слов справа-слева. Возможно, имеет смысл идти от N-ой выделенной сущности до N+2, и включать всё до неё. \n",
    "\n",
    "Чем может помочь синтаксический анализ?\n",
    "\n",
    "Чем поможет морфологический анализ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"/Users/dmitrys/anaconda2/lib/python2.7/site-packages/\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pymorphy2\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "m = Mystem()\n",
    "def lemmatize(text, mystem=m):\n",
    "    try:\n",
    "        return \"\".join(m.lemmatize(text)).strip()  \n",
    "    except:\n",
    "        return \" \"\n",
    "\n",
    "import string\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_progress(sequence, every=None, size=None, name='Items'):\n",
    "    from ipywidgets import IntProgress, HTML, VBox\n",
    "    from IPython.display import display\n",
    "\n",
    "    is_iterator = False\n",
    "    if size is None:\n",
    "        try:\n",
    "            size = len(sequence)\n",
    "        except TypeError:\n",
    "            is_iterator = True\n",
    "    if size is not None:\n",
    "        if every is None:\n",
    "            if size <= 200:\n",
    "                every = 1\n",
    "            else:\n",
    "                every = int(size / 200)     # every 0.5%\n",
    "    else:\n",
    "        assert every is not None, 'sequence is iterator, set every'\n",
    "\n",
    "    if is_iterator:\n",
    "        progress = IntProgress(min=0, max=1, value=1)\n",
    "        progress.bar_style = 'info'\n",
    "    else:\n",
    "        progress = IntProgress(min=0, max=size, value=0)\n",
    "    label = HTML()\n",
    "    box = VBox(children=[label, progress])\n",
    "    display(box)\n",
    "\n",
    "    index = 0\n",
    "    try:\n",
    "        for index, record in enumerate(sequence, 1):\n",
    "            if index == 1 or index % every == 0:\n",
    "                if is_iterator:\n",
    "                    label.value = '{name}: {index} / ?'.format(\n",
    "                        name=name,\n",
    "                        index=index\n",
    "                    )\n",
    "                else:\n",
    "                    progress.value = index\n",
    "                    label.value = u'{name}: {index} / {size}'.format(\n",
    "                        name=name,\n",
    "                        index=index,\n",
    "                        size=size\n",
    "                    )\n",
    "            yield record\n",
    "    except:\n",
    "        progress.bar_style = 'danger'\n",
    "        raise\n",
    "    else:\n",
    "        progress.bar_style = 'success'\n",
    "        progress.value = index\n",
    "        label.value = \"{name}: {index}\".format(\n",
    "            name=name,\n",
    "            index=str(index or '?')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def commit():\n",
    "    ! git add NLP_summerschool.ipynb\n",
    "    ! git commit -m \"and now for something comepletely different\"\n",
    "    ! git push -u origin master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master a019586] working......\n",
      " 1 file changed, 1186 insertions(+), 2257 deletions(-)\n",
      " rewrite NLP_summerschool.ipynb (63%)\n",
      "Counting objects: 3, done.\n",
      "Delta compression using up to 4 threads.\n",
      "Compressing objects: 100% (3/3), done.\n",
      "Writing objects: 100% (3/3), 2.06 KiB | 0 bytes/s, done.\n",
      "Total 3 (delta 2), reused 0 (delta 0)\n",
      "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
      "To https://github.com/DmitrySerg/HSE-NLP-Summer-School.git\n",
      "   0c8cdaf..a019586  master -> master\n",
      "Branch master set up to track remote branch master from origin.\n"
     ]
    }
   ],
   "source": [
    "commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0,
     5,
     10,
     35,
     53,
     81,
     84
    ],
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def loadAnswer(number):\n",
    "    with open(\"Texts/art{}.opin.txt\".format(number)) as f:\n",
    "        d = f.read()\n",
    "    return d\n",
    "\n",
    "def loadText(number):\n",
    "    with open(\"Texts/art{}.txt\".format(number)) as f:\n",
    "        d = f.read()\n",
    "    return d\n",
    "\n",
    "def transformAnnotation(number):\n",
    "    \"\"\"\n",
    "    Given number loads txt file with annotation \n",
    "    Returns DataFrame with transformed annotation\n",
    "    \"\"\"\n",
    "    with open(\"Texts/art{}.ann\".format(number)) as f:\n",
    "        d = f.read()\n",
    "\n",
    "    d = d.split(\"\\n\")\n",
    "\n",
    "    for i in range(len(d)):\n",
    "        d[i] = d[i].split(\"\\t\")\n",
    "\n",
    "    d = pd.DataFrame(d)\n",
    "    d.drop([0], axis=1, inplace=True)\n",
    "    d = pd.concat([d, pd.DataFrame(d[1].apply(lambda x: x.split()).tolist())], axis=1)\n",
    "    d.columns = [\"to_delete\", \"entity\", \"entity_car\", \"pos_1\", \"pos_2\"]\n",
    "    d.drop([\"to_delete\"], axis=1, inplace=True)\n",
    "    d[\"entity\"] = d[\"entity\"].apply(lambda x: x.strip(\"\\r\"))\n",
    "    d[\"entity\"] = d[\"entity\"].apply(lambda x: x.decode(\"utf8\"))\n",
    "    \n",
    "    d = d[~d.entity.isin([\"Unknown\", \"Author\"])].entity.reset_index(drop=True)\n",
    "    \n",
    "    return d\n",
    "\n",
    "def transformAnswer(number):\n",
    "    \"\"\"\n",
    "    Given number loads txt file with answer \n",
    "    Returns DataFrame with transformed answer\n",
    "    \"\"\"\n",
    "    answ = loadAnswer(number)\n",
    "    answ = answ.split(\"\\n\")\n",
    "\n",
    "    for i in range(len(answ)):\n",
    "        answ[i] = answ[i].split(\",\")\n",
    "\n",
    "    answ = pd.DataFrame(answ)\n",
    "    answ.columns = [\"entity_1\", \"entity_2\", \"attitude\", \"time\"]\n",
    "    answ.dropna(inplace=True)\n",
    "    answ.time = answ.time.apply(lambda x: x.strip(\"\\r\"))\n",
    "    \n",
    "    return answ\n",
    "\n",
    "def loadRuSentiLex():\n",
    "    \"\"\"\n",
    "    Loads the RuSentiLex 2017 dictionary\n",
    "    Returns data frame with [\"word\", \"tag\", \"word_lemmatized\", \"tone\", \"certainty\"]\n",
    "    \"\"\"\n",
    "    with open(\"RuSentiLex2017_revised_2.txt\") as f:\n",
    "        Rusentilex = f.read().decode(\"cp1251\").encode(\"utf8\")\n",
    "        Rusentilex = Rusentilex[1510:]\n",
    "\n",
    "    Rusentilex = Rusentilex.split(\"\\n\")\n",
    "    for i, item in enumerate(Rusentilex):\n",
    "        Rusentilex[i] = item.split(\",\")\n",
    "\n",
    "    for i in Rusentilex:\n",
    "        if len(i) < 5:\n",
    "            Rusentilex.remove(i)\n",
    "\n",
    "    Rusentilex = pd.DataFrame(Rusentilex)\n",
    "    Rusentilex.drop([5, 6, 7], axis=1, inplace=True)\n",
    "    Rusentilex.columns = [\"word\", \"tag\", \"word_lemmatized\", \"tone\", \"certainty\"]\n",
    "    Rusentilex[\"certainty\"] = Rusentilex[\"certainty\"].apply(lambda x: x.strip('\\r'))\n",
    "    \n",
    "    \n",
    "    Rusentilex[\"tone\"] = Rusentilex[\"tone\"].apply(lambda x: x.strip(' '))\n",
    "    Rusentilex[\"certainty\"] = Rusentilex[\"certainty\"].apply(lambda x: x.strip(' '))\n",
    "    \n",
    "    return Rusentilex\n",
    "\n",
    "def cleanString(myString):\n",
    "    return myString.translate(None, string.punctuation).decode('utf-8')\n",
    "\n",
    "def getSentimentCertainity(word):\n",
    "    word = lemmatize(word)\n",
    "    try:\n",
    "        tone, certainty =  Rusentilex[[\"tone\", \"certainty\"]][Rusentilex.word.isin([word])].values[0]\n",
    "    except:\n",
    "        try:\n",
    "            tone, certainty =  Rusentilex[[\"tone\", \"certainty\"]][Rusentilex.word.isin([word.encode(\"utf8\")])].values[0]\n",
    "        except:\n",
    "            tone, certainty = \"NaN\", \"NaN\"\n",
    "    return tone, certainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![](http://cathyreisenwitz.com/wp-content/uploads/2016/01/no.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Словарь РуСентиЛекс\n",
    "\n",
    "Структура: \n",
    "- 1 слово или словосочетание,\n",
    "- 2 Часть речи или синтаксический тип группы,\n",
    "- 3 слово или словосочетание в лемматизированной форме, \n",
    "- 4 Тональность: позитивная (positive), негативная(negative), нейтральная (neutral) или неопределеная оценка, зависит от контекста (positive/negative),\n",
    "- 5 Источник: оценка (opinion), чувство (feeling), факт (fact),\n",
    "- 6 Если тональность отличается для разных значений многозначного слова, то перечисляются все значения слова по тезаурусу РуТез и дается отсылка на сооветствующее понятие - имя понятия в кавычках."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Rusentilex = loadRuSentiLex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "      <th>word_lemmatized</th>\n",
       "      <th>tone</th>\n",
       "      <th>certainty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>аборт</td>\n",
       "      <td>Noun</td>\n",
       "      <td>аборт</td>\n",
       "      <td>negative</td>\n",
       "      <td>fact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>абортивный</td>\n",
       "      <td>Adj</td>\n",
       "      <td>абортивный</td>\n",
       "      <td>negative</td>\n",
       "      <td>fact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>абракадабра</td>\n",
       "      <td>Noun</td>\n",
       "      <td>абракадабра</td>\n",
       "      <td>negative</td>\n",
       "      <td>opinion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>абсурд</td>\n",
       "      <td>Noun</td>\n",
       "      <td>абсурд</td>\n",
       "      <td>negative</td>\n",
       "      <td>opinion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>абсурдность</td>\n",
       "      <td>Noun</td>\n",
       "      <td>абсурдность</td>\n",
       "      <td>negative</td>\n",
       "      <td>opinion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word    tag word_lemmatized      tone certainty\n",
       "0        аборт   Noun           аборт  negative      fact\n",
       "1   абортивный    Adj      абортивный  negative      fact\n",
       "2  абракадабра   Noun     абракадабра  negative   opinion\n",
       "3       абсурд   Noun          абсурд  negative   opinion\n",
       "4  абсурдность   Noun     абсурдность  negative   opinion"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rusentilex.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('negative', 'fact')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getSentimentCertainity(u\"абортами\".encode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def Text_to_dict(number):\n",
    "    t = loadText(number)\n",
    "    text_dict = t.split(\"\\n\\n\")\n",
    "    text_dict = {i:parag for i, parag in enumerate(text_dict)}\n",
    "    sentence_dict = {i:{} for i in range(len(text_dict))}\n",
    "\n",
    "    for key, paragraph in text_dict.iteritems():\n",
    "        paragraph = paragraph.split(\"{Author, Unknown}\")\n",
    "        for sent_numbet, sentence in enumerate(paragraph):\n",
    "\n",
    "            sentence = cleanString(sentence)\n",
    "            if len(sentence) != 0:\n",
    "                sentence_dict[key][sent_numbet] = sentence\n",
    "                #tknzr.tokenize()\n",
    "    return sentence_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getEntityPositions(text, entities):\n",
    "    \"\"\"\n",
    "    This bad boy finds (hopefully) all entities in the text\n",
    "    Collects their exact locations and returns a neat Dataframe with them\n",
    "    \"\"\"\n",
    "    \n",
    "    ENTITIES = pd.DataFrame(columns=[\"entity\", \"paragraph\", \"sentence\", \"loc_start\", \"loc_end\"])\n",
    "    \n",
    "    for paragraph in text:\n",
    "        for sentence in text[paragraph]:\n",
    "            for entity in list(entities.unique()):\n",
    "                if \" \" + entity + \" \" in \" \" + text[paragraph][sentence] +  \" \":\n",
    "                    loc_start = text[paragraph][sentence].find(entity)\n",
    "                    loc_end = loc_start + len(entity)\n",
    "                    \n",
    "                    to_append =   {\"entity\":entity, \n",
    "                                   \"paragraph\":paragraph, \n",
    "                                   \"sentence\":sentence,\n",
    "                                   \"loc_start\":loc_start,\n",
    "                                   \"loc_end\":loc_end}\n",
    "                    \n",
    "                    ENTITIES = ENTITIES.append(to_append, ignore_index = True)\n",
    "    ENTITIES = ENTITIES.drop_duplicates()\n",
    "    ENTITIES = ENTITIES.sort_values(by=[\"paragraph\", \"sentence\", \"loc_start\"])\n",
    "    ENTITIES = ENTITIES.reset_index(drop=True)\n",
    "    return ENTITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distances(loc1, loc2):\n",
    "    \"\"\"\n",
    "    returns distances from one entity to another\n",
    "    \"\"\"\n",
    "    paragraph = ENTITIES.loc[loc2].paragraph - ENTITIES.loc[loc1].paragraph\n",
    "    sentence = ENTITIES.loc[loc2].sentence - ENTITIES.loc[loc1].sentence\n",
    "    return paragraph, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMorphCase(word):\n",
    "    p = morph.parse(u\"аборт\")[0]\n",
    "    return p.tag.case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getPairs(ENTITIES, text, n_entities=2):\n",
    "    N_ENTITIES = n_entities # глобальный параметр, сколько сущностей будем перебирать для текущей\n",
    "    PAIRS = pd.DataFrame(columns=[\"entity_1\", \"entity_2\", \"sentence\"])\n",
    "    ent_1_number = 0\n",
    "    while ent_1_number < len(ENTITIES):\n",
    "\n",
    "        entity_1 = ENTITIES.entity[ent_1_number]\n",
    "        for ent_2_number in range(N_ENTITIES):\n",
    "            try:            \n",
    "                ent_2_number = ent_1_number+ent_2_number+1\n",
    "                entity_2 = ENTITIES.entity[ent_2_number]\n",
    "                par_dist, sent_dist = distances(ent_1_number, ent_2_number)\n",
    "                if lemmatize(entity_1) != lemmatize(entity_2): # не равны друг другу\n",
    "                    if par_dist == 0: # в одном абзаце\n",
    "                        if sent_dist == 0: # в одном предложении\n",
    "                            PAIRS = PAIRS.append({\"entity_1\":entity_1,\n",
    "                                                  \"entity_2\":entity_2,\n",
    "                                                  \"sentence\":text[ENTITIES.loc[ent_1_number].paragraph][ENTITIES.loc[ent_1_number].sentence]},\n",
    "                                                  ignore_index=True)\n",
    "                    else:\n",
    "                        #ent_1_number+=1\n",
    "                        PAIRS = PAIRS.append({\"entity_1\":\"Author\",\n",
    "                                                  \"entity_2\":entity_1,\n",
    "                                                  \"sentence\":text[ENTITIES.loc[ent_1_number].paragraph][ENTITIES.loc[ent_1_number].sentence]},\n",
    "                                                  ignore_index=True)\n",
    "            except:\n",
    "                continue\n",
    "        ent_1_number += 1\n",
    "        \n",
    "    return PAIRS.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSentenceSentiment(sentence):\n",
    "    cur_sentence = sentence\n",
    "    cur_sentence = tknzr.tokenize(cur_sentence)\n",
    "    for word in cur_sentence:\n",
    "        if (len(word)<3) or (word.isdigit()):\n",
    "            cur_sentence.remove(word)\n",
    "    sentiment = []\n",
    "\n",
    "    for word in cur_sentence:\n",
    "        sentiment.append(getSentimentCertainity(word)[0])\n",
    "        \n",
    "    sentiment = [x for x in sentiment if x!=\"NaN\"]\n",
    "    \n",
    "    result = [sentiment.count(\"negative\"), sentiment.count(\"positive\")]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepareData(number, n_entities=3):\n",
    "    \n",
    "    \"\"\"\n",
    "    Функция пока не работает, потому что зафакапил области видимости\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"Loaging stuff\"\"\"\n",
    "    \n",
    "    text = Text_to_dict(number)\n",
    "    entities = transformAnnotation(number)\n",
    "    answer = transformAnswer(number) \n",
    "    \n",
    "    ENTITIES = getEntityPositions(text=text, entities=entities)\n",
    "\n",
    "    \"\"\"Calculating stuff\"\"\"\n",
    "    \n",
    "    PAIRS = getPairs(ENTITIES=ENTITIES, text=text, n_entities=n_entities)\n",
    "    print(PAIRS.head())\n",
    "    answer[\"pair\"] = answer.entity_1.apply(lambda x: lemmatize(x).lower())+\",\"+\\\n",
    "                     answer.entity_2.apply(lambda x: lemmatize(x).lower())\n",
    "    PAIRS[\"pair\"] = PAIRS.entity_1.apply(lambda x: lemmatize(x).lower())+\",\"+\\\n",
    "                    PAIRS.entity_2.apply(lambda x: lemmatize(x).lower())\n",
    "    PAIRS[\"pair\"] = PAIRS[\"pair\"].apply(lambda x: x.encode(\"utf8\"))\n",
    "\n",
    "    PAIRS.drop([\"entity_1\", \"entity_2\"], axis=1, inplace=True)\n",
    "    PAIRS = PAIRS.groupby([\"pair\"]).sum().reset_index()\n",
    "\n",
    "    answer = answer.drop([\"entity_1\", \"entity_2\", \"time\"], axis=1)\n",
    "\n",
    "    \"\"\"Merging stuff\"\"\"\n",
    "    \n",
    "    PAIRS = PAIRS.merge(answer, on='pair', how=\"left\")\n",
    "    PAIRS = PAIRS.fillna(\"neutral\")\n",
    "    \n",
    "    \"\"\"Getting sentiment from stuff\"\"\"\n",
    "    try:\n",
    "        sent = PAIRS.sentence.apply(lambda x: getSentenceSentiment(x))\n",
    "        sent = pd.DataFrame(list(sent))\n",
    "        sent.columns = [\"negative\", \"positive\"]\n",
    "    except:\n",
    "        sent = \n",
    "        \n",
    "    PAIRS = pd.concat([PAIRS, sent], axis=1)\n",
    "    PAIRS = PAIRS[PAIRS.negative+PAIRS.positive != 0]\n",
    "    \n",
    "    return PAIRS, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [
     1
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no file with number 0\n",
      "(272, 3)\n",
      "(336, 3)\n",
      "(401, 3)\n",
      "(519, 3)\n",
      "(599, 3)\n",
      "(658, 3)\n",
      "(671, 3)\n",
      "(721, 3)\n",
      "no file with number 9\n",
      "(866, 3)\n",
      "(920, 3)\n",
      "(978, 3)\n",
      "(1033, 3)\n",
      "(1334, 3)\n",
      "(1387, 3)\n",
      "(1408, 3)\n",
      "(1504, 3)\n",
      "(1540, 3)\n",
      "(1587, 3)\n",
      "(1598, 3)\n",
      "(1677, 3)\n",
      "no file with number 22\n",
      "(1776, 3)\n",
      "(1841, 3)\n",
      "(1901, 3)\n",
      "no file with number 26\n",
      "(2069, 3)\n",
      "(2127, 3)\n",
      "(2165, 3)\n",
      "(2219, 3)\n",
      "(2247, 3)\n",
      "(2372, 3)\n",
      "(2412, 3)\n",
      "(2497, 3)\n",
      "(2657, 3)\n",
      "(2725, 3)\n",
      "(2764, 3)\n",
      "(2801, 3)\n",
      "(2863, 3)\n",
      "(2907, 3)\n",
      "(3044, 3)\n",
      "(3102, 3)\n",
      "(3159, 3)\n",
      "(3340, 3)\n",
      "(3434, 3)\n",
      "no file with number 46\n",
      "no file with number 47\n",
      "no file with number 48\n",
      "no file with number 49\n"
     ]
    }
   ],
   "source": [
    "FINAL = pd.DataFrame()\n",
    "for number in log_progress(range(50)):\n",
    "    try:\n",
    "        n_entities=3\n",
    "\n",
    "\n",
    "        \"\"\"Loaging stuff\"\"\"\n",
    "\n",
    "        text = Text_to_dict(number)\n",
    "        entities = transformAnnotation(number)\n",
    "        answer = transformAnswer(number) \n",
    "\n",
    "        ENTITIES = getEntityPositions(text=text, entities=entities)\n",
    "\n",
    "        \"\"\"Calculating stuff\"\"\"\n",
    "\n",
    "        PAIRS = getPairs(ENTITIES=ENTITIES, text=text, n_entities=n_entities)\n",
    "        answer[\"pair\"] = answer.entity_1.apply(lambda x: lemmatize(x).lower())+\",\"+\\\n",
    "                         answer.entity_2.apply(lambda x: lemmatize(x).lower())\n",
    "        PAIRS[\"pair\"] = PAIRS.entity_1.apply(lambda x: lemmatize(x).lower())+\",\"+\\\n",
    "                        PAIRS.entity_2.apply(lambda x: lemmatize(x).lower())\n",
    "        PAIRS[\"pair\"] = PAIRS[\"pair\"].apply(lambda x: x.encode(\"utf8\"))\n",
    "\n",
    "        PAIRS.drop([\"entity_1\", \"entity_2\"], axis=1, inplace=True)\n",
    "        PAIRS = PAIRS.groupby([\"pair\"]).sum().reset_index()\n",
    "\n",
    "        answer = answer.drop([\"entity_1\", \"entity_2\", \"time\"], axis=1)\n",
    "\n",
    "        \"\"\"Merging stuff\"\"\"\n",
    "\n",
    "        PAIRS = PAIRS.merge(answer, on='pair', how=\"left\")\n",
    "        PAIRS = PAIRS.fillna(\"neutral\")\n",
    "        FINAL = FINAL.append(PAIRS, ignore_index=True)\n",
    "        print(FINAL.shape)\n",
    "    except:\n",
    "        print(\"no file with number {}\".format(number))\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FINAL.attitude = FINAL.attitude.apply(lambda x: x.strip(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral    0.903320\n",
       "neg        0.058532\n",
       "pos        0.038148\n",
       "Name: attitude, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FINAL.attitude.value_counts()/len(FINAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FINAL.to_pickle(\"FINAL_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FINAL.pair = FINAL.pair.apply(lambda x: x.decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
