{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Файл labeledTrainData.tsv содержит тексты, которые размеченны по классам\n",
    "train = pd.read_csv('labeledTrainData.tsv', header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "# Файл testData.tsv содержит тексты, по которым нужно выдать предсказания\n",
    "test = pd.read_csv('testData.tsv', header=0,  delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "# Обратим внимание на unlabeledTrainData.tsv\n",
    "# Для данного файла нет меток, к какому классу относятся его тексты\n",
    "# Так же организаторы не ждут предсказания для по классам для текстов из данного файла\n",
    "# В нем представленны тексты того же посола, что и остальные\n",
    "# А значит добавив его в обучение word2vec мы улучшим знание нашемй модели о \"мире\"\n",
    "unsup = pd.read_csv('unlabeledTrainData.tsv', header=0,  delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию которая для представленного текста:\n",
    "* меняет n't на not (по желанию)\n",
    "* приводит к нижнему регистру\n",
    "* делит на слова\n",
    "* удаляет стоп-слова (по желанию)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "def text_to_wordlist(text):\n",
    "    text = re.sub('n\\'t', ' not', text)\n",
    "    \n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    words = text.lower().split()\n",
    "\n",
    "    #stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Протестируем данную функцию\n",
    "\n",
    "text_to_wordlist(\"\"\"\n",
    "Tyrone Garland <s>(born 1992)</s> is an American professional basketball player \n",
    "who last played with the National <b>Basketball</b> League of Canada's Mississauga Power\n",
    "\"\"\")\n",
    "\n",
    "# Посмотрим что результатом будет list, элементы которого - слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию которая для представленного текста:\n",
    "* удаляет html теги\n",
    "* производит деления на предложения\n",
    "* каждое предложение делит на слова (применяя выше написанную функцию)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "def text_to_sentences(text):\n",
    "    text = BeautifulSoup(text).get_text()\n",
    "    \n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(text.strip())\n",
    "    \n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(text_to_wordlist(raw_sentence))\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Протестируем данную функцию\n",
    "\n",
    "text_to_sentences(\"\"\"\n",
    "Sherlock Holmes is a four-act play written by <p/> William Gillette and Sir Arthur Conan Doyle, \n",
    "based on Conan Doyle's eponymous character. It drew material from the stories \n",
    "<s>\"A Scandal in Bohemia\"</s>, \"The Final Problem\", and A Study in Scarlet, pitting Holmes \n",
    "against Professor Moriarty and reinventing the character of Irene Adler as a new love \n",
    "interest named Alice Faulkner. This play introduced the phrase \"Elementary, my dear Watson\" \n",
    "and Holmes' curved pipe.\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# Посмотрим что результатом будет list, элементы которого - list, элементы которого - слова\n",
    "# т.е. получаем list list'ов (список списков)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так давайте соберем все имеющиеся тексты в подобню структуру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sentences = []\n",
    "\n",
    "for review in train[\"review\"]:\n",
    "    sentences += text_to_sentences(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for review in unsup[\"review\"]:\n",
    "    sentences += text_to_sentences(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for review in test[\"review\"]:\n",
    "    sentences += text_to_sentences(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# В конце сбора sentences будет list list'ов (список списков) - как и пример выше.\n",
    "# (Повторюсь) каждый элемент списка sentences - предложение, но представленное в виде списка слов - потому список\n",
    "\n",
    "# выведем количество элементов этого массива (оно же - количество предложений во всех текста)\n",
    "print len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# а так же посмотрим на сам массив\n",
    "print '\\t Первый элемент массива'\n",
    "print sentences[0]\n",
    "print \n",
    "print '\\t Второй элемент массива'\n",
    "print sentences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим же теперь можель Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# симортируем соответствующую функцию из модуля gensim, который должен быть установлен\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec \n",
    "\n",
    "# список параметров, которые можно менять по вашему желанию\n",
    "num_features = 300  # итоговая размерность вектора каждого слова\n",
    "min_word_count = 5  # минимальная частотность слова, чтобы оно попало в модель\n",
    "num_workers = 8     # количество ядер вашего процессора, чтоб запустить обучение в несколько потоков\n",
    "context = 10        # размер окна \n",
    "downsampling = 1e-3 # внутренняя метрика модели\n",
    "\n",
    "model = Word2Vec(sentences, workers=num_workers, size=num_features,\n",
    "                 min_count=min_word_count, window=context, sample=downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Финализируем нашу модель. Ее нельзя будет доучить теперь, но она начнет занимать гораздо меньше места\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Натренеровав модель, получили представление каждого слова в семантическом пространстве (часто называют \"псевдо\" семантическое пространство)\n",
    "\n",
    "Попробуем популярный пример: QUEEN + MAN - KING = ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=['queen', 'man'], negative=['king'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилось WOMAN, что логично :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Посмотрим слова, которые очень похожи на слово MOVIE\n",
    "\n",
    "model.most_similar('movie')\n",
    "\n",
    "# Попробуем и другие слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar('awful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar('big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar('mail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# так же взглянем на функцию doesnt_match, она покажет лишнее слово в массив\n",
    "\n",
    "model.doesnt_match(['berlin', 'moscow', 'africa', 'riga', 'madrid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.doesnt_match(['man', 'woman', 'child', 'dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# давайте просмотрим вектор одного из слов\n",
    "# его длину\n",
    "print len(model['moon'])\n",
    "\n",
    "# и сам вектор\n",
    "print model['moon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Словарь - все слова которые участвуют в модели можно просмотреть так\n",
    "model.index2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Т.к. мы хотим классифицировать не слова, а тексты, надо перевести тексты в вектора (представить в виде фич)\n",
    "Один из простых методов - сложить все вектора слов входящих в текст и поделить на число входящих слов.\n",
    "Напишем функцию, которая:\n",
    "* создает нулевой вектор - это будет результирующий вектор\n",
    "* идем по всем словам в тексте, если слово есть в моделе:\n",
    "  * увеличиваем счетчик слов\n",
    "  * прибавим вектор слова к результирующему вектору\n",
    "* поделим все координаты на число слов по которым, вектора которых мы прибавляли к результирующему вектору"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def text_to_vec(words, model, size):\n",
    "    text_vec = np.zeros((size,), dtype=\"float32\")\n",
    "    n_words = 0\n",
    "\n",
    "    index2word_set = set(model.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words = n_words + 1\n",
    "            text_vec = np.add(text_vec, model[word])\n",
    "    \n",
    "    if n_words != 0:\n",
    "        text_vec /= n_words\n",
    "    return text_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию, которая на входе получает список всех текстов, а на выходе отдает список вектор каждого текста - что является прямоугольной матрицей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def texts_to_vecs(texts, model, size):\n",
    "    texts_vecs = np.zeros((len(texts), size), dtype=\"float32\")\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        texts_vecs[i] = text_to_vec(text, model, size)\n",
    "\n",
    "    return texts_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим что функция texts_to_vecs принимает не просто тексты, а список всех слов текста. \n",
    "\n",
    "(!!!) Внимание: не список списков (там где сначала делили на предложения, а предложения на слова), а обычный линейный список\n",
    "\n",
    "Но у нас есть функции, которые переводят 1) текст в список предложений, 2) предложение в список слов\n",
    "\n",
    "Может показаться, что можно использовать 2ую функцию, но придется тогда ее переписать, потому как теги у нас удаляются лишь в первой функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Поступим иначе, в python есть возможность развернуть двухмерный массив в одномерный, вот пример\n",
    "\n",
    "temp_list = [[1, 2, 3], [4, 5], [6, 7, 8, 9]]\n",
    "print sum(temp_list, [])\n",
    "\n",
    "# магия :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Мы же такой возможностью воспользуемся, зная что функция text_to_sentences возвращает список списков\n",
    "\n",
    "list_of_list_of_words = text_to_sentences(\"\"\"\n",
    "Sherlock Holmes is a four-act play written by <p/> William Gillette and Sir Arthur Conan Doyle, \n",
    "based on Conan Doyle's eponymous character. It drew material from the stories \n",
    "<s>\"A Scandal in Bohemia\"</s>, \"The Final Problem\", and A Study in Scarlet, pitting Holmes \n",
    "against Professor Moriarty and reinventing the character of Irene Adler as a new love \n",
    "interest named Alice Faulkner. This play introduced the phrase \"Elementary, my dear Watson\" \n",
    "and Holmes' curved pipe.\n",
    "\"\"\")\n",
    "\n",
    "print '\\t длина list_of_list_of_words'\n",
    "print len(list_of_list_of_words)\n",
    "print '\\t первый элемент list_of_list_of_words'\n",
    "print list_of_list_of_words[0]\n",
    "\n",
    "list_of_words = sum(list_of_list_of_words, [])\n",
    "\n",
    "print '\\t длина list_of_words'\n",
    "print len(list_of_words)\n",
    "print '\\t первый элемент list_of_words'\n",
    "print list_of_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# действительно работает, сделаем для всех текстов из train\n",
    "train_like_word_list = [sum(text_to_sentences(text), []) for text in train['review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_vecs = texts_to_vecs(train_like_word_list, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# сделаем тоже самое для test\n",
    "test_like_word_list = [sum(text_to_sentences(text), []) for text in test['review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_vecs = texts_to_vecs(test_like_word_list, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Воспользуемся train_vecs, test_vecs, train[\"sentiment\"] \n",
    "#    как матрица фичей обучающей выборки, матрица фичей тестовой выборки, таргет для обучающей выборки соответственно\n",
    "\n",
    "# Стандартный случайный лес в таком случае\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=100, n_jobs=8)\n",
    "forest = forest.fit(train_vecs, train[\"sentiment\"])\n",
    "predict = forest.predict(test_vecs)\n",
    "\n",
    "# И вот задача решена"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но что если для получения результирующего вектора не складываеть вектора, а пойти другим способом.\n",
    "\n",
    "Кластеризуем все слова на 1000 классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# В model.syn0 храняться все вектора. Кластеризируем их!\n",
    "\n",
    "print 'Размер ', model.syn0.shape\n",
    "print 'Вектор векторов ', model.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Кластеризируем все слова. \n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "word_vectors = model.syn0\n",
    "# Число кластеров установим в 10000. Для этого числа нет \"серебряной пули\". Для каждого случая лучше подойдет разная\n",
    "num_clusters = 10000\n",
    "\n",
    "# Начнем кластеризацию, учитывая что классов много, количество векторов (по сути слов) много,\n",
    "#   все это будет происходит продолжительное время. Можно сходить за чаем.\n",
    "kmeans_clustering = KMeans(n_clusters=num_clusters)\n",
    "idx = kmeans_clustering.fit_predict(word_vectors)\n",
    "\n",
    "# в idx будут храниться номера классов для каждого слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# создадим структуру dict (словарь): слово -> класс\n",
    "word_centroid_map = dict(zip(model.index2word, idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# выведем представителей первых 10 классов и посмотрим на адекватность произошедшей кластеризации\n",
    "\n",
    "for cluster in xrange(0,10):\n",
    "    print cluster\n",
    "    words = []\n",
    "    for i in xrange(0, len(word_centroid_map.values())):\n",
    "        if word_centroid_map.values()[i] == cluster:\n",
    "            words.append(word_centroid_map.keys()[i])\n",
    "    print words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует несколько подходов к работе с кластерами слов. Рассмотрим 2 примера\n",
    "* 1) посчитаем для каждого текста вектор сколько его слов встретилось в каждом кластере. Т.е. для каждого текста будет вектор размера равного числу кластеров, значениями вектора будут натуральные числа\n",
    "* 2) Посчитаем усредненных удаленность векторов текстов от каждого центройда всех кластеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# для первого случая напишем функцию, на вход которой поступает текст, представленный в виде списка всех его слов\n",
    "#    смотрим в каком кластере находится каждое слово \n",
    "#    и увеличиваем соответствующую ячейку (ответственную за этот кластер) на 1\n",
    "\n",
    "\n",
    "def create_bag_of_centroids(wordlist, word_centroid_map, num_centroids):\n",
    "    bag_of_centroids = np.zeros(num_centroids, dtype=\"float32\")\n",
    "    set_word_centroid_map = set(word_centroid_map.keys())\n",
    "    \n",
    "    for word in wordlist:\n",
    "        if word in set_word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    \n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Но нам нужно это не для одного текста, а для всех текстов обучающей и тестовой выборки\n",
    "# Сделаем это\n",
    "\n",
    "train_vecs_centroids = np.zeros((train[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "for i, text in enumerate(train_like_word_list):\n",
    "    train_vecs_centroids[i] = create_bag_of_centroids(text, word_centroid_map, num_clusters)\n",
    "\n",
    "test_vecs_centroids = np.zeros((test[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "for i, text in enumerate(test_like_word_list):\n",
    "    test_vecs_centroids[i] = create_bag_of_centroids(text, word_centroid_map, num_clusters)\n",
    "    \n",
    "# Результатом будет матрицы для обучающих и тестовых текстов, что будет матрицей фич обучающей и тестовой выборок соотв."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# стандартный случайный лес на полученных матрицах\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=100, n_jobs=8)\n",
    "forest = forest.fit(train_vecs_centroids, train[\"sentiment\"])\n",
    "predict = forest.predict(test_vecs_centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# а теперь попробуем посчитать растояние от центройдов\n",
    "\n",
    "train_vecs_centroids_dist = np.zeros((train[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "for i, vec in enumerate(train_vecs):\n",
    "    for j, center in enumerate(kmeans_clustering.cluster_centers_):\n",
    "        train_vecs_centroids_dist[i][j] = distance.euclidean(vec, center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_vecs_centroids_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_vecs_centroids_dist = np.zeros((test[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "for i, vec in enumerate(test_vecs):\n",
    "    for j, center in enumerate(kmeans_clustering.cluster_centers_):\n",
    "        test_vecs_centroids_dist[i][j] = distance.euclidean(vec, center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_vecs_centroids_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# стандартный случайный лес на полученных матрицах (кто бы сомневался)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=100, n_jobs=8)\n",
    "forest = forest.fit(train_vecs_centroids_dist, train[\"sentiment\"])\n",
    "predict = forest.predict(test_vecs_centroids_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
